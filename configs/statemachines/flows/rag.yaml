# RAG Flow Configuration
# This configuration defines the RAG workflow parameters and settings

enabled: false

# RAG system configuration
rag:
  # Embedding model settings
  embeddings:
    model_type: "custom"  # openai, custom
    model_name: "sentence-transformers/all-MiniLM-L6-v2"
    api_key: null
    base_url: "localhost:8001"
    num_dimensions: 384
    batch_size: 32
    max_retries: 3
    timeout: 30.0

  # LLM model settings
  llm:
    model_type: "custom"  # openai, custom
    model_name: "microsoft/DialoGPT-medium"
    host: "localhost"
    port: 8000
    api_key: null
    max_tokens: 2048
    temperature: 0.7
    top_p: 0.9
    frequency_penalty: 0.0
    presence_penalty: 0.0
    stop: null
    stream: false

  # Vector store settings
  vector_store:
    store_type: "chroma"  # chroma, neo4j, postgres, pinecone, weaviate, qdrant
    connection_string: null
    host: "localhost"
    port: 8000
    database: null
    collection_name: "research_docs"
    api_key: null
    embedding_dimension: 384
    distance_metric: "cosine"
    index_type: "hnsw"

  # Document processing settings
  chunk_size: 1000
  chunk_overlap: 200
  max_context_length: 4000
  enable_reranking: false
  reranker_model: null

  # Document sources
  file_sources: []
  database_sources: []
  web_sources: []

  # Processing settings
  batch_size: 32
  max_retries: 3
  timeout: 30.0

  # Output settings
  output_format: "detailed"  # detailed, summary, minimal
  include_sources: true
  include_scores: true

# VLLM deployment settings
vllm_deployment:
  auto_start: true
  health_check_interval: 30
  max_retries: 3

  # LLM server settings
  llm_server:
    model_name: "microsoft/DialoGPT-medium"
    host: "0.0.0.0"
    port: 8000
    gpu_memory_utilization: 0.9
    max_model_len: 4096
    dtype: "auto"
    trust_remote_code: false
    tensor_parallel_size: 1
    pipeline_parallel_size: 1

  # Embedding server settings
  embedding_server:
    model_name: "sentence-transformers/all-MiniLM-L6-v2"
    host: "0.0.0.0"
    port: 8001
    gpu_memory_utilization: 0.9
    max_model_len: 512
    dtype: "auto"
    trust_remote_code: false
    tensor_parallel_size: 1
    pipeline_parallel_size: 1
