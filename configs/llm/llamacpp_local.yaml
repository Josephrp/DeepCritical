# llama.cpp local server configuration
# Compatible with llama.cpp OpenAI-compatible API mode

# Basic connection settings
provider: "llamacpp"
model_name: "llama"
base_url: "http://localhost:8080/v1"
api_key: null  # llama.cpp doesn't require API key by default

# Generation parameters
generation:
  temperature: 0.7
  max_tokens: 512
  top_p: 0.9
  frequency_penalty: 0.0
  presence_penalty: 0.0

# Connection settings
timeout: 60.0
max_retries: 3
retry_delay: 1.0
