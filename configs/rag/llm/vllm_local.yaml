# VLLM Local LLM Configuration
model_type: "custom"
model_name: "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
host: "localhost"
port: 8000
api_key: null
max_tokens: 2048
temperature: 0.7
top_p: 0.9
frequency_penalty: 0.0
presence_penalty: 0.0
stop: null
stream: false
